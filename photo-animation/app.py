"""
Application d'animation de photos r√©aliste
Utilise des mod√®les d'IA pour animer des photos de fa√ßon r√©aliste
Compatible avec Hugging Face Spaces
"""

import gradio as gr
import torch
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import os
from typing import Optional, Tuple
import warnings
warnings.filterwarnings("ignore")

# Configuration
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Configuration du cache des mod√®les
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(ROOT_DIR)  # Remonter d'un niveau depuis photo-animation
MODELS_CACHE_DIR = os.path.join(PROJECT_ROOT, "ai-models-cache", "huggingface")

# Configurer les variables d'environnement pour utiliser le cache personnalis√©
os.environ["HF_HOME"] = MODELS_CACHE_DIR
os.environ["HUGGINGFACE_HUB_CACHE"] = MODELS_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = MODELS_CACHE_DIR

print(f"[INFO] Cache des modeles configure: {MODELS_CACHE_DIR}")

# Essayer d'importer diffusers, utiliser fallback si non disponible
DIFFUSERS_AVAILABLE = False
StableDiffusionImg2ImgPipeline = None

try:
    # D√©sactiver xformers pour √©viter les erreurs de DLL
    os.environ["XFORMERS_DISABLED"] = "1"
    os.environ["DIFFUSERS_NO_ADAPTER"] = "1"
    from diffusers import StableDiffusionImg2ImgPipeline
    DIFFUSERS_AVAILABLE = True
    print("[OK] Diffusers disponible")
except Exception as e:
    DIFFUSERS_AVAILABLE = False
    StableDiffusionImg2ImgPipeline = None
    print(f"[WARNING] Diffusers non disponible ({type(e).__name__}), utilisation du mode fallback")
    print("   L'application fonctionnera avec OpenCV pour l'animation")

class PhotoAnimator:
    """Classe pour animer des photos de fa√ßon r√©aliste"""
    
    def __init__(self):
        self.pipeline = None
        self.device = DEVICE
        self.use_diffusers = DIFFUSERS_AVAILABLE
        if self.use_diffusers:
            self._load_model()
    
    def _load_model(self):
        """Charge le mod√®le d'animation"""
        if not self.use_diffusers:
            return
            
        try:
            print(f"Chargement du mod√®le sur {self.device}...")
            # Utilisation d'un mod√®le Img2Img pour l'animation
            model_id = "runwayml/stable-diffusion-v1-5"
            
            # Utiliser le cache personnalis√©
            cache_dir = MODELS_CACHE_DIR
            
            print(f"[INFO] Utilisation du cache: {cache_dir}")
            
            self.pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
                model_id,
                cache_dir=cache_dir,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False
            )
            self.pipeline = self.pipeline.to(self.device)
            if self.device == "cuda":
                self.pipeline.enable_attention_slicing()
                self.pipeline.enable_model_cpu_offload()
            print("[OK] Modele charge avec succes depuis le cache!")
        except Exception as e:
            print(f"[ERREUR] Erreur lors du chargement du modele: {e}")
            import traceback
            traceback.print_exc()
            print("Utilisation d'un mode de fallback...")
            self.pipeline = None
            self.use_diffusers = False
    
    def animate_photo(
        self,
        image: Image.Image,
        animation_type: str = "subtle",
        strength: float = 0.5,
        num_frames: int = 8
    ) -> Image.Image:
        """
        Anime une photo de fa√ßon r√©aliste
        
        Args:
            image: Image PIL √† animer
            animation_type: Type d'animation ("subtle", "moderate", "strong")
            strength: Force de l'animation (0.0 √† 1.0)
            num_frames: Nombre de frames pour l'animation
        
        Returns:
            Image anim√©e avec effet de mouvement
        """
        # Convertir l'image en format appropri√©
        image = image.convert("RGB")
        original_size = image.size
        
        # Utiliser le pipeline si disponible, sinon fallback
        if self.pipeline is not None and self.use_diffusers:
            try:
                # Ajuster la taille si n√©cessaire (limite pour la m√©moire)
                max_size = 768
                width, height = image.size
                if width > max_size or height > max_size:
                    ratio = min(max_size / width, max_size / height)
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)
                    resized_image = image.resize((new_width, new_height), Image.LANCZOS)
                else:
                    resized_image = image
                
                # Cr√©er un prompt pour l'animation
                prompts = {
                    "subtle": "realistic photo, natural movement, subtle animation, high quality, detailed",
                    "moderate": "realistic photo, gentle motion, smooth animation, high quality, detailed",
                    "strong": "realistic photo, dynamic movement, animated, high quality, detailed"
                }
                
                prompt = prompts.get(animation_type, prompts["subtle"])
                negative_prompt = "blurry, distorted, low quality, artifacts, deformed, ugly"
                
                # G√©n√©rer une version anim√©e
                result = self.pipeline(
                    prompt=prompt,
                    image=resized_image,
                    num_inference_steps=25,
                    guidance_scale=7.5,
                    strength=min(strength, 0.8),  # Limiter la force pour pr√©server l'original
                    negative_prompt=negative_prompt
                ).images[0]
                
                # Redimensionner au format original
                if result.size != original_size:
                    result = result.resize(original_size, Image.LANCZOS)
                
                # Appliquer un post-traitement pour am√©liorer la qualit√©
                result = self._post_process(result, image)
                
                return result
                
            except Exception as e:
                print(f"Erreur lors de l'animation avec diffusers: {e}")
                return self._fallback_animation(image, animation_type, strength)
        else:
            # Mode fallback: applique un effet de mouvement r√©aliste
            return self._fallback_animation(image, animation_type, strength)
    
    def _fallback_animation(self, image: Image.Image, animation_type: str, strength: float) -> Image.Image:
        """M√©thode de fallback utilisant OpenCV et PIL pour cr√©er un effet d'animation r√©aliste"""
        try:
            # Convertir PIL en numpy
            img_array = np.array(image.convert("RGB"))
            original = img_array.copy()
            
            # Cr√©er un effet de mouvement r√©aliste
            if animation_type == "subtle":
                # L√©g√®re translation avec blur motion subtil
                tx, ty = int(3 * strength), int(2 * strength)
                M = np.float32([[1, 0, tx], [0, 1, ty]])
                moved = cv2.warpAffine(img_array, M, (img_array.shape[1], img_array.shape[0]), 
                                      borderMode=cv2.BORDER_REPLICATE)
                # L√©g√®re am√©lioration de contraste et saturation
                result_array = self._enhance_image(moved, strength * 0.3)
                
            elif animation_type == "moderate":
                # Rotation l√©g√®re avec translation
                center = (img_array.shape[1] // 2, img_array.shape[0] // 2)
                angle = 0.8 * strength
                M = cv2.getRotationMatrix2D(center, angle, 1.0)
                rotated = cv2.warpAffine(img_array, M, (img_array.shape[1], img_array.shape[0]),
                                        borderMode=cv2.BORDER_REPLICATE)
                # Translation suppl√©mentaire
                tx, ty = int(5 * strength), int(3 * strength)
                M2 = np.float32([[1, 0, tx], [0, 1, ty]])
                moved = cv2.warpAffine(rotated, M2, (img_array.shape[1], img_array.shape[0]),
                                      borderMode=cv2.BORDER_REPLICATE)
                result_array = self._enhance_image(moved, strength * 0.5)
                
            else:  # strong
                # Combinaison de rotation, translation et effets visuels
                center = (img_array.shape[1] // 2, img_array.shape[0] // 2)
                angle = 1.5 * strength
                M = cv2.getRotationMatrix2D(center, angle, 1.0)
                rotated = cv2.warpAffine(img_array, M, (img_array.shape[1], img_array.shape[0]),
                                        borderMode=cv2.BORDER_REPLICATE)
                tx, ty = int(8 * strength), int(5 * strength)
                M2 = np.float32([[1, 0, tx], [0, 1, ty]])
                moved = cv2.warpAffine(rotated, M2, (img_array.shape[1], img_array.shape[0]),
                                      borderMode=cv2.BORDER_REPLICATE)
                result_array = self._enhance_image(moved, strength * 0.7)
            
            # M√©langer avec l'original pour pr√©server la qualit√©
            blend_factor = 0.85
            result_array = (blend_factor * result_array + (1 - blend_factor) * original).astype(np.uint8)
            
            # Convertir back en PIL
            result = Image.fromarray(result_array)
            return result
            
        except Exception as e:
            print(f"Erreur dans le fallback: {e}")
            return image
    
    def _enhance_image(self, img_array: np.ndarray, factor: float) -> np.ndarray:
        """Am√©liore l'image avec contraste et saturation"""
        try:
            # Conversion BGR pour OpenCV
            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
            
            # Ajuster la luminosit√© et le contraste
            alpha = 1.0 + factor * 0.2  # Contraste
            beta = factor * 10  # Luminosit√©
            enhanced = cv2.convertScaleAbs(img_bgr, alpha=alpha, beta=beta)
            
            # Ajuster la saturation
            hsv = cv2.cvtColor(enhanced, cv2.COLOR_BGR2HSV)
            hsv[:, :, 1] = cv2.multiply(hsv[:, :, 1], 1.0 + factor * 0.3)
            enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # Reconvertir en RGB
            return cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)
        except:
            return img_array
    
    def _post_process(self, result: Image.Image, original: Image.Image) -> Image.Image:
        """Post-traitement pour am√©liorer la qualit√©"""
        try:
            # S'assurer que les tailles correspondent
            if result.size != original.size:
                original = original.resize(result.size, Image.LANCZOS)
            
            # M√©langer avec l'original pour pr√©server les d√©tails
            result_array = np.array(result).astype(np.float32)
            original_array = np.array(original).astype(np.float32)
            
            # Blend 75% r√©sultat, 25% original pour pr√©server les d√©tails
            blended = (0.75 * result_array + 0.25 * original_array).astype(np.uint8)
            
            # Am√©lioration l√©g√®re de la nettet√©
            result_pil = Image.fromarray(blended)
            enhancer = ImageEnhance.Sharpness(result_pil)
            result_pil = enhancer.enhance(1.1)
            
            return result_pil
        except Exception as e:
            print(f"Erreur dans post_process: {e}")
            return result


# Initialiser l'animateur
animator = PhotoAnimator()


def animate_image(
    image: Optional[Image.Image],
    animation_type: str,
    strength: float,
    num_frames: int
) -> Tuple[Image.Image, str]:
    """
    Fonction principale pour l'interface Gradio
    
    Args:
        image: Image √† animer
        animation_type: Type d'animation
        strength: Force de l'animation
        num_frames: Nombre de frames
    
    Returns:
        Tuple (image anim√©e, message de statut)
    """
    if image is None:
        return None, "‚ö†Ô∏è Veuillez t√©l√©charger une image"
    
    try:
        # Animer l'image
        animated = animator.animate_photo(
            image=image,
            animation_type=animation_type,
            strength=strength,
            num_frames=num_frames
        )
        
        return animated, "‚úÖ Animation termin√©e avec succ√®s!"
        
    except Exception as e:
        error_msg = f"‚ùå Erreur: {str(e)}"
        print(error_msg)
        return None, error_msg


# Interface Gradio
def create_interface():
    """Cr√©e l'interface Gradio"""
    
    with gr.Blocks(title="Animation de Photos R√©aliste", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # üé¨ Animation de Photos R√©aliste
            
            Animez vos photos de fa√ßon r√©aliste en utilisant l'intelligence artificielle.
            T√©l√©chargez une photo et choisissez le type d'animation souhait√©.
            
            **Note:** Cette application utilise des mod√®les d'IA pour cr√©er des animations r√©alistes.
            """
        )
        
        with gr.Row():
            with gr.Column(scale=1):
                input_image = gr.Image(
                    label="üì∑ Photo √† animer",
                    type="pil",
                    height=400
                )
                
                animation_type = gr.Radio(
                    choices=["subtle", "moderate", "strong"],
                    value="subtle",
                    label="Type d'animation",
                    info="Subtle: mouvement l√©ger | Moderate: mouvement mod√©r√© | Strong: mouvement prononc√©"
                )
                
                strength = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.5,
                    step=0.1,
                    label="Force de l'animation",
                    info="Contr√¥le l'intensit√© de l'animation"
                )
                
                num_frames = gr.Slider(
                    minimum=4,
                    maximum=16,
                    value=8,
                    step=1,
                    label="Nombre de frames",
                    info="Nombre d'images pour l'animation (pour versions futures)"
                )
                
                animate_btn = gr.Button("‚ú® Animer la photo", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                output_image = gr.Image(
                    label="üé¨ Photo anim√©e",
                    type="pil",
                    height=400
                )
                
                status = gr.Textbox(
                    label="Statut",
                    value="En attente d'une image...",
                    interactive=False
                )
        
        # Exemples
        gr.Markdown("### üì∏ Exemples")
        gr.Examples(
            examples=[],
            inputs=[input_image],
            label="T√©l√©chargez votre propre image pour commencer"
        )
        
        # √âv√©nements
        animate_btn.click(
            fn=animate_image,
            inputs=[input_image, animation_type, strength, num_frames],
            outputs=[output_image, status]
        )
        
        input_image.upload(
            fn=lambda img: (img, "‚úÖ Image charg√©e, pr√™t √† animer!"),
            inputs=[input_image],
            outputs=[input_image, status]
        )
    
    return demo


# Lancer l'application
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Demarrage de l'application d'animation de photos...")
    print("=" * 60)
    try:
        demo = create_interface()
        print("\n[INFO] Interface creee avec succes")
        print("[INFO] Demarrage du serveur Gradio...")
        print("[INFO] L'application sera accessible sur: http://localhost:7860")
        print("=" * 60 + "\n")
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            show_error=True
        )
    except Exception as e:
        print(f"\n[ERREUR] Impossible de demarrer l'application: {e}")
        import traceback
        traceback.print_exc()
        input("\nAppuyez sur Entree pour quitter...")
