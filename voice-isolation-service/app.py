"""
Application Gradio pour l'isolation vocale avec Demucs
Inspir√©e de https://huggingface.co/spaces/abidlabs/music-separation
"""

import os
import tempfile
import shutil
from pathlib import Path
import gradio as gr
import torch
import torchaudio
from demucs.pretrained import get_model
from demucs.audio import convert_audio
from demucs.apply import apply_model
import numpy as np
from pydub import AudioSegment
import soundfile as sf

# Configuration
MODEL_NAME = "htdemucs"  # Mod√®le Demucs v4 (HT = Hybrid Transformer)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Charger le mod√®le au d√©marrage
print(f"üîÑ Chargement du mod√®le Demucs ({MODEL_NAME}) sur {DEVICE}...")
try:
    model = get_model(MODEL_NAME)
    model.to(DEVICE)
    model.eval()
    
    # Obtenir le sample_rate du mod√®le (g√©rer BagOfModels et HTDemucs)
    MODEL_SAMPLE_RATE = 44100  # Valeur par d√©faut pour htdemucs
    MODEL_CHANNELS = 2  # Valeur par d√©faut (st√©r√©o)
    
    if hasattr(model, 'sample_rate'):
        MODEL_SAMPLE_RATE = model.sample_rate
    elif hasattr(model, 'models') and len(model.models) > 0:
        # Si c'est un BagOfModels, prendre le sample_rate du premier mod√®le
        if hasattr(model.models[0], 'sample_rate'):
            MODEL_SAMPLE_RATE = model.models[0].sample_rate
    elif hasattr(model, 'sr'):
        # Certains mod√®les utilisent 'sr' au lieu de 'sample_rate'
        MODEL_SAMPLE_RATE = model.sr
    
    # Obtenir le nombre de canaux audio (pas les canaux internes du mod√®le)
    # Pour htdemucs, c'est toujours 2 (st√©r√©o)
    if hasattr(model, 'audio_channels'):
        channels = model.audio_channels
        if channels in [1, 2]:  # Mono ou st√©r√©o seulement
            MODEL_CHANNELS = channels
    elif hasattr(model, 'models') and len(model.models) > 0:
        if hasattr(model.models[0], 'audio_channels'):
            channels = model.models[0].audio_channels
            if channels in [1, 2]:
                MODEL_CHANNELS = channels
    # Sinon, utiliser la valeur par d√©faut de 2 (st√©r√©o)
    
    print(f"‚úÖ Mod√®le charg√© avec succ√®s sur {DEVICE}")
    print(f"   Sample rate: {MODEL_SAMPLE_RATE} Hz, Channels: {MODEL_CHANNELS}")
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
    model = None
    MODEL_SAMPLE_RATE = 44100
    MODEL_CHANNELS = 2

def separate_sources(audio_file, stem="vocals"):
    """
    S√©pare les sources audio en utilisant Demucs
    
    Args:
        audio_file: Fichier audio upload√© (tuple avec (sample_rate, audio_data) ou filepath)
        stem: Source √† extraire ("vocals", "drums", "bass", "other", ou "all")
    
    Returns:
        Tuple de fichiers audio s√©par√©s
    """
    if model is None:
        return None, None, None, None, "‚ùå Mod√®le non charg√©. Veuillez red√©marrer l'application."
    
    if audio_file is None:
        return None, None, None, None, "‚ö†Ô∏è Veuillez uploader un fichier audio."
    
    try:
        # Dossier pour stocker les fichiers upload√©s
        upload_dir = Path("/app/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # G√©rer les diff√©rents formats d'entr√©e Gradio
        if isinstance(audio_file, tuple):
            # Format (sample_rate, audio_data)
            sr, audio_data = audio_file
            # Convertir numpy array en tensor
            if isinstance(audio_data, np.ndarray):
                wav = torch.from_numpy(audio_data).float()
                if len(wav.shape) == 1:
                    wav = wav.unsqueeze(0)  # Ajouter dimension channel
                elif wav.shape[0] > 1:
                    wav = wav.mean(dim=0, keepdim=True)  # Convertir en mono
            else:
                wav = torch.tensor(audio_data).float()
                if len(wav.shape) == 1:
                    wav = wav.unsqueeze(0)
            
            # Sauvegarder le fichier upload√© dans /app/uploads pour tra√ßabilit√©
            import time
            timestamp = int(time.time())
            uploaded_file_path = upload_dir / f"uploaded_{timestamp}.wav"
            # Convertir le tensor en numpy pour sauvegarder
            audio_np = wav.cpu().numpy()
            if len(audio_np.shape) == 2:
                audio_np = audio_np.T  # Transposer pour soundfile
            elif len(audio_np.shape) == 1:
                audio_np = audio_np.reshape(-1, 1)
            sf.write(str(uploaded_file_path), audio_np, sr)
            print(f"üíæ Fichier upload√© sauvegard√©: {uploaded_file_path}")
        else:
            # Format filepath
            print(f"üéµ Traitement du fichier: {audio_file}")
            
            # Copier le fichier upload√© dans /app/uploads pour tra√ßabilit√©
            import time
            timestamp = int(time.time())
            source_file = Path(audio_file)
            file_ext = source_file.suffix or ".wav"
            uploaded_file_path = upload_dir / f"uploaded_{timestamp}{file_ext}"
            
            # Copier le fichier
            shutil.copy2(str(audio_file), str(uploaded_file_path))
            print(f"üíæ Fichier upload√© copi√©: {uploaded_file_path}")
            
            # V√©rifier l'extension du fichier
            file_ext = Path(audio_file).suffix.lower()
            formats_supported_by_torchaudio = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac']
            
            # Si le format n'est pas directement support√© (ex: WMA), convertir avec pydub
            if file_ext not in formats_supported_by_torchaudio:
                print(f"‚ö†Ô∏è Format {file_ext} d√©tect√©, conversion en WAV temporaire...")
                try:
                    # Charger avec pydub (supporte plus de formats)
                    audio = AudioSegment.from_file(audio_file)
                    # Cr√©er un fichier temporaire WAV
                    temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                    audio.export(temp_wav.name, format='wav')
                    audio_file = temp_wav.name
                    print(f"‚úÖ Fichier converti en WAV: {audio_file}")
                except Exception as e:
                    print(f"‚ùå Erreur lors de la conversion: {e}")
                    return None, None, None, None, f"‚ùå Format {file_ext} non support√©. Veuillez convertir en MP3, WAV, FLAC, OGG ou M4A."
            
            try:
                # Essayer de charger avec torchaudio (sans torchcodec)
                wav, sr = torchaudio.load(audio_file, backend="soundfile")
            except Exception as e:
                # Si √©chec, utiliser librosa comme fallback
                print(f"‚ö†Ô∏è torchaudio √©chou√©, utilisation de librosa: {e}")
                import librosa
                audio_data, sr = librosa.load(audio_file, sr=None, mono=False)
                if len(audio_data.shape) == 1:
                    audio_data = audio_data.reshape(1, -1)  # Ajouter dimension channel
                wav = torch.from_numpy(audio_data).float()
            
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)  # Convertir en mono si st√©r√©o
        
        # Convertir au format attendu par Demucs
        wav = convert_audio(wav, sr, MODEL_SAMPLE_RATE, MODEL_CHANNELS)
        wav = wav.to(DEVICE)
        
        print(f"üîä Audio charg√©: {wav.shape}, sample_rate: {MODEL_SAMPLE_RATE}")
        
        # S√©parer les sources
        print("üîÑ S√©paration des sources en cours...")
        with torch.no_grad():
            # Utiliser apply_model au lieu d'un appel direct
            sources = apply_model(model, wav[None], device=DEVICE, split=True, overlap=0.25, progress=False)
            sources = sources[0]  # Retirer dimension batch
        
        # Les sources sont dans l'ordre: [drums, bass, other, vocals]
        drums, bass, other, vocals = sources
        
        # Utiliser le dossier outputs configur√© dans Docker
        output_dir = Path("/app/outputs")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Cr√©er un sous-dossier avec timestamp pour cette s√©paration
        import time
        timestamp = int(time.time())
        session_dir = output_dir / f"session_{timestamp}"
        session_dir.mkdir(parents=True, exist_ok=True)
        
        results = {}
        
        # Fonction helper pour sauvegarder avec soundfile (√©vite TorchCodec)
        def save_audio_safe(path, audio_tensor, sample_rate):
            """Sauvegarde un tensor audio en WAV en utilisant soundfile directement"""
            # Convertir en numpy array et transposer si n√©cessaire
            audio_np = audio_tensor.cpu().numpy()
            if len(audio_np.shape) == 3:
                audio_np = audio_np[0]  # Retirer dimension batch
            if len(audio_np.shape) == 2:
                # Transposer de (channels, samples) √† (samples, channels)
                audio_np = audio_np.T
            elif len(audio_np.shape) == 1:
                # Mono: ajouter dimension channel
                audio_np = audio_np.reshape(-1, 1)
            sf.write(str(path), audio_np, sample_rate)
        
        if stem == "all" or stem == "vocals":
            vocals_path = session_dir / "vocals.wav"
            save_audio_safe(vocals_path, vocals, MODEL_SAMPLE_RATE)
            results["vocals"] = str(vocals_path)
        
        if stem == "all" or stem == "drums":
            drums_path = session_dir / "drums.wav"
            save_audio_safe(drums_path, drums, MODEL_SAMPLE_RATE)
            results["drums"] = str(drums_path)
        
        if stem == "all" or stem == "bass":
            bass_path = session_dir / "bass.wav"
            save_audio_safe(bass_path, bass, MODEL_SAMPLE_RATE)
            results["bass"] = str(bass_path)
        
        if stem == "all" or stem == "other":
            other_path = session_dir / "other.wav"
            save_audio_safe(other_path, other, MODEL_SAMPLE_RATE)
            results["other"] = str(other_path)
        
        # Retourner les r√©sultats selon le stem demand√©
        if stem == "all":
            return (
                results.get("vocals"),
                results.get("drums"),
                results.get("bass"),
                results.get("other"),
                "‚úÖ S√©paration compl√®te termin√©e!"
            )
        else:
            result_file = results.get(stem)
            if result_file:
                return result_file, None, None, None, f"‚úÖ {stem.capitalize()} extrait avec succ√®s!"
            else:
                return None, None, None, None, f"‚ùå Erreur lors de l'extraction de {stem}"
        
    except Exception as e:
        error_msg = f"‚ùå Erreur: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return None, None, None, None, error_msg

# Interface Gradio
with gr.Blocks() as demo:
    
    gr.HTML("""
    <div class="main-header">
        <h1>üé§ Isolation Vocale par IA</h1>
        <p>S√©parez la voix, la batterie, la basse et les autres instruments de vos fichiers audio</p>
        <p style="font-size: 0.9em; opacity: 0.9;">Bas√© sur Demucs v4 - Mod√®le HTDemucs</p>
    </div>
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            audio_input = gr.Audio(
                label="üìÅ Fichier Audio",
                type="filepath",
                sources=["upload", "microphone"],
                format="wav"  # WAV pour meilleure compatibilit√©
            )
            
            stem_choice = gr.Radio(
                choices=[
                    ("üé§ Voix uniquement", "vocals"),
                    ("ü•Å Batterie uniquement", "drums"),
                    ("üé∏ Basse uniquement", "bass"),
                    ("üéπ Autres instruments", "other"),
                    ("üéµ Toutes les sources", "all")
                ],
                value="vocals",
                label="Source √† extraire",
                info="Choisissez quelle source vous souhaitez isoler"
            )
            
            separate_btn = gr.Button(
                "üéØ S√©parer les sources",
                variant="primary",
                size="lg"
            )
            
            status = gr.Textbox(
                label="Statut",
                interactive=False,
                value="‚è≥ Pr√™t √† traiter un fichier audio"
            )
        
        with gr.Column(scale=1):
            if True:  # Pour permettre plusieurs outputs conditionnels
                vocals_output = gr.Audio(
                    label="üé§ Voix isol√©e",
                    type="filepath",
                    visible=True
                )
                drums_output = gr.Audio(
                    label="ü•Å Batterie isol√©e",
                    type="filepath",
                    visible=False
                )
                bass_output = gr.Audio(
                    label="üé∏ Basse isol√©e",
                    type="filepath",
                    visible=False
                )
                other_output = gr.Audio(
                    label="üéπ Autres instruments isol√©s",
                    type="filepath",
                    visible=False
                )
    
    # Fonction pour g√©rer la visibilit√© des outputs
    def update_outputs_visibility(stem):
        if stem == "all":
            return (
                gr.update(visible=True),  # vocals
                gr.update(visible=True),  # drums
                gr.update(visible=True),  # bass
                gr.update(visible=True),  # other
            )
        else:
            return (
                gr.update(visible=(stem == "vocals")),  # vocals
                gr.update(visible=(stem == "drums")),  # drums
                gr.update(visible=(stem == "bass")),  # bass
                gr.update(visible=(stem == "other")),  # other
            )
    
    # Fonction wrapper pour la s√©paration
    def process_separation(audio_file, stem):
        result = separate_sources(audio_file, stem)
        
        vocals, drums, bass, other, status_msg = result
        
        if stem == "all":
            return (
                vocals,
                drums,
                bass,
                other,
                status_msg,
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(visible=True),
            )
        else:
            visibility = update_outputs_visibility(stem)
            return (
                vocals if stem == "vocals" else None,
                drums if stem == "drums" else None,
                bass if stem == "bass" else None,
                other if stem == "other" else None,
                status_msg,
                *visibility
            )
    
    # Connecter les √©v√©nements
    stem_choice.change(
        fn=update_outputs_visibility,
        inputs=[stem_choice],
        outputs=[vocals_output, drums_output, bass_output, other_output]
    )
    
    separate_btn.click(
        fn=process_separation,
        inputs=[audio_input, stem_choice],
        outputs=[
            vocals_output,
            drums_output,
            bass_output,
            other_output,
            status,
            vocals_output,
            drums_output,
            bass_output,
            other_output
        ]
    )
    
    # Exemples
    gr.Examples(
        examples=[],
        inputs=audio_input,
        label="Exemples (√† venir)"
    )
    
    # Informations
    gr.Markdown("""
    ### ‚ÑπÔ∏è Comment √ßa fonctionne ?
    
    Cette application utilise **Demucs v4** (Hybrid Transformer), un mod√®le d'IA de pointe pour la s√©paration de sources audio.
    
    **Fonctionnalit√©s :**
    - üé§ **Isolation vocale** : Extrait uniquement la voix d'un enregistrement
    - ü•Å **Isolation de batterie** : S√©pare la batterie du reste
    - üé∏ **Isolation de basse** : Extrait la ligne de basse
    - üéπ **Autres instruments** : Isole les autres instruments (guitares, synth√©s, etc.)
    - üéµ **S√©paration compl√®te** : Extrait toutes les sources en une fois
    
    **Formats support√©s :** MP3, WAV, M4A, OGG, FLAC, WMA (converti automatiquement)
    
    **Note :** 
    - Le traitement peut prendre quelques minutes selon la longueur du fichier audio
    - Les formats non support√©s par les navigateurs (comme WMA) sont automatiquement convertis
    - Les fichiers de sortie sont toujours en format WAV pour une compatibilit√© maximale
    """)

# Lancer l'application
if __name__ == "__main__":
    # Configurer les dossiers pour Gradio
    upload_dir = Path("/app/uploads")
    output_dir = Path("/app/outputs")
    upload_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        file_directories=[str(upload_dir), str(output_dir)],  # Dossiers accessibles via Gradio
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .main-header {
            text-align: center;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        """
    )
