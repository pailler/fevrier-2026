"""
Application Gradio pour l'isolation vocale avec Demucs
Inspir√©e de https://huggingface.co/spaces/abidlabs/music-separation
"""

import os
import tempfile
import shutil
from pathlib import Path
import gradio as gr
import torch
import torchaudio
from demucs.pretrained import get_model
from demucs.audio import convert_audio
from demucs.apply import apply_model
import numpy as np
from pydub import AudioSegment
import soundfile as sf
import matplotlib
matplotlib.use('Agg')  # Backend non-interactif pour √©viter les probl√®mes
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
import base64
from io import BytesIO

# Configuration
MODEL_NAME = "htdemucs"  # Mod√®le Demucs v4 (HT = Hybrid Transformer)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Charger le mod√®le au d√©marrage
print(f"üîÑ Chargement du mod√®le Demucs ({MODEL_NAME}) sur {DEVICE}...")
try:
    model = get_model(MODEL_NAME)
    model.to(DEVICE)
    model.eval()
    
    # Obtenir le sample_rate du mod√®le (g√©rer BagOfModels et HTDemucs)
    MODEL_SAMPLE_RATE = 44100  # Valeur par d√©faut pour htdemucs
    MODEL_CHANNELS = 2  # Valeur par d√©faut (st√©r√©o)
    
    if hasattr(model, 'sample_rate'):
        MODEL_SAMPLE_RATE = model.sample_rate
    elif hasattr(model, 'models') and len(model.models) > 0:
        # Si c'est un BagOfModels, prendre le sample_rate du premier mod√®le
        if hasattr(model.models[0], 'sample_rate'):
            MODEL_SAMPLE_RATE = model.models[0].sample_rate
    elif hasattr(model, 'sr'):
        # Certains mod√®les utilisent 'sr' au lieu de 'sample_rate'
        MODEL_SAMPLE_RATE = model.sr
    
    # Obtenir le nombre de canaux audio (pas les canaux internes du mod√®le)
    # Pour htdemucs, c'est toujours 2 (st√©r√©o)
    if hasattr(model, 'audio_channels'):
        channels = model.audio_channels
        if channels in [1, 2]:  # Mono ou st√©r√©o seulement
            MODEL_CHANNELS = channels
    elif hasattr(model, 'models') and len(model.models) > 0:
        if hasattr(model.models[0], 'audio_channels'):
            channels = model.models[0].audio_channels
            if channels in [1, 2]:
                MODEL_CHANNELS = channels
    # Sinon, utiliser la valeur par d√©faut de 2 (st√©r√©o)
    
    print(f"‚úÖ Mod√®le charg√© avec succ√®s sur {DEVICE}")
    print(f"   Sample rate: {MODEL_SAMPLE_RATE} Hz, Channels: {MODEL_CHANNELS}")
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
    model = None
    MODEL_SAMPLE_RATE = 44100
    MODEL_CHANNELS = 2

def get_audio_info(audio_file):
    """
    R√©cup√®re les informations sur le fichier audio (dur√©e, format, taille)
    """
    if audio_file is None:
        return "Aucun fichier s√©lectionn√©"
    
    try:
        if isinstance(audio_file, tuple):
            sr, audio_data = audio_file
            duration = len(audio_data) / sr if isinstance(audio_data, np.ndarray) else 0
            format_info = "WAV (upload)"
            size_info = f"{len(audio_data) * 4 / 1024 / 1024:.2f} MB" if isinstance(audio_data, np.ndarray) else "N/A"
        else:
            file_path = Path(audio_file)
            size_info = f"{file_path.stat().st_size / 1024 / 1024:.2f} MB"
            format_info = file_path.suffix.upper().replace('.', '') or "WAV"
            
            # Obtenir la dur√©e
            try:
                import librosa
                duration = librosa.get_duration(path=audio_file)
            except:
                try:
                    wav, sr = torchaudio.load(audio_file, backend="soundfile")
                    duration = wav.shape[1] / sr
                except:
                    duration = 0
        
        duration_str = f"{int(duration // 60)}:{int(duration % 60):02d}" if duration > 0 else "N/A"
        return f"üìä Format: {format_info} | Dur√©e: {duration_str} | Taille: {size_info}"
    except Exception as e:
        return f"‚ÑπÔ∏è Informations non disponibles: {str(e)}"

def generate_waveform_image(audio_file_path, title="Waveform", color="#9333ea"):
    """
    G√©n√®re une visualisation waveform d'un fichier audio
    """
    if audio_file_path is None or not Path(audio_file_path).exists():
        return None
    
    try:
        # Charger l'audio
        wav, sr = torchaudio.load(audio_file_path, backend="soundfile")
        audio_data = wav.numpy()
        
        # Convertir en mono si st√©r√©o
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=0)
        
        # Limiter le nombre d'√©chantillons pour la visualisation (max 10000 points)
        max_samples = 10000
        if len(audio_data) > max_samples:
            step = len(audio_data) // max_samples
            audio_data = audio_data[::step]
        
        # Cr√©er la figure
        fig = Figure(figsize=(12, 3), facecolor='white')
        ax = fig.add_subplot(111)
        
        # Tracer la waveform
        time_axis = np.linspace(0, len(audio_data) / sr, len(audio_data))
        ax.plot(time_axis, audio_data, color=color, linewidth=0.5, alpha=0.8)
        ax.fill_between(time_axis, audio_data, 0, color=color, alpha=0.3)
        
        # Configuration
        ax.set_xlabel('Temps (s)', fontsize=10)
        ax.set_ylabel('Amplitude', fontsize=10)
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.set_xlim(0, len(audio_data) / sr)
        
        # Convertir en base64 pour l'affichage HTML
        buf = BytesIO()
        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight', facecolor='white')
        buf.seek(0)
        img_base64 = base64.b64encode(buf.read()).decode('utf-8')
        buf.close()
        plt.close(fig)
        
        return f"data:image/png;base64,{img_base64}"
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration de la waveform: {e}")
        return None

def create_track_html(audio_file, title, icon, color, waveform_img=None):
    """
    Cr√©e le HTML pour afficher une piste avec waveform et lecteur audio
    """
    if audio_file is None:
        return f"""
        <div style="padding: 20px; text-align: center; color: #666;">
            <p>{icon} {title} - Aucun fichier disponible</p>
        </div>
        """
    
    # Gradio utilise /file= pour servir les fichiers depuis les dossiers configur√©s
    # Le chemin doit √™tre relatif au dossier outputs
    audio_path = Path(audio_file)
    if audio_path.is_absolute():
        # Extraire le chemin relatif depuis /app/outputs
        if '/app/outputs' in str(audio_path):
            relative_path = str(audio_path).split('/app/outputs/')[-1]
            audio_url = f"/file=outputs/{relative_path}"
        else:
            # Utiliser le nom du fichier seulement
            audio_url = f"/file=outputs/{audio_path.name}"
    else:
        audio_url = f"/file={audio_file}"
    
    waveform_html = f'<img src="{waveform_img}" style="width: 100%; height: auto; border-radius: 8px; margin-bottom: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" />' if waveform_img else ""
    
    return f"""
    <div style="background: linear-gradient(135deg, {color}15, {color}05); border: 2px solid {color}30; border-radius: 12px; padding: 20px; margin-bottom: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h3 style="color: {color}; margin-top: 0; display: flex; align-items: center; gap: 10px; font-size: 18px;">
            <span style="font-size: 28px;">{icon}</span>
            <span>{title}</span>
        </h3>
        {waveform_html}
        <div style="margin-top: 15px; background: white; padding: 10px; border-radius: 8px;">
            <audio controls style="width: 100%;" preload="metadata">
                <source src="{audio_url}" type="audio/wav">
                Votre navigateur ne supporte pas l'√©l√©ment audio.
            </audio>
        </div>
        <div style="margin-top: 10px; text-align: center;">
            <a href="{audio_url}" download style="display: inline-block; padding: 10px 20px; background: {color}; color: white; text-decoration: none; border-radius: 6px; font-weight: bold; transition: all 0.3s; box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
                üì• T√©l√©charger cette piste
            </a>
        </div>
    </div>
    """

def separate_sources(audio_file, stem="vocals", progress=None):
    """
    S√©pare les sources audio en utilisant Demucs
    
    Args:
        audio_file: Fichier audio upload√© (tuple avec (sample_rate, audio_data) ou filepath)
        stem: Source √† extraire ("vocals", "drums", "bass", "other", ou "all")
        progress: Objet de progression Gradio (optionnel)
    
    Returns:
        Tuple de fichiers audio s√©par√©s
    """
    if model is None:
        return None, None, None, None, "‚ùå Mod√®le non charg√©. Veuillez red√©marrer l'application.", ""
    
    if audio_file is None:
        return None, None, None, None, "‚ö†Ô∏è Veuillez uploader un fichier audio.", ""
    
    try:
        # Dossier pour stocker les fichiers upload√©s
        upload_dir = Path("/app/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # G√©rer les diff√©rents formats d'entr√©e Gradio
        if isinstance(audio_file, tuple):
            # Format (sample_rate, audio_data)
            sr, audio_data = audio_file
            # Convertir numpy array en tensor
            if isinstance(audio_data, np.ndarray):
                wav = torch.from_numpy(audio_data).float()
                if len(wav.shape) == 1:
                    wav = wav.unsqueeze(0)  # Ajouter dimension channel
                elif wav.shape[0] > 1:
                    wav = wav.mean(dim=0, keepdim=True)  # Convertir en mono
            else:
                wav = torch.tensor(audio_data).float()
                if len(wav.shape) == 1:
                    wav = wav.unsqueeze(0)
            
            # Sauvegarder le fichier upload√© dans /app/uploads pour tra√ßabilit√©
            import time
            timestamp = int(time.time())
            uploaded_file_path = upload_dir / f"uploaded_{timestamp}.wav"
            # Convertir le tensor en numpy pour sauvegarder
            audio_np = wav.cpu().numpy()
            if len(audio_np.shape) == 2:
                audio_np = audio_np.T  # Transposer pour soundfile
            elif len(audio_np.shape) == 1:
                audio_np = audio_np.reshape(-1, 1)
            sf.write(str(uploaded_file_path), audio_np, sr)
            print(f"üíæ Fichier upload√© sauvegard√©: {uploaded_file_path}")
        else:
            # Format filepath
            print(f"üéµ Traitement du fichier: {audio_file}")
            
            # Copier le fichier upload√© dans /app/uploads pour tra√ßabilit√©
            import time
            timestamp = int(time.time())
            source_file = Path(audio_file)
            file_ext = source_file.suffix or ".wav"
            uploaded_file_path = upload_dir / f"uploaded_{timestamp}{file_ext}"
            
            # Copier le fichier
            shutil.copy2(str(audio_file), str(uploaded_file_path))
            print(f"üíæ Fichier upload√© copi√©: {uploaded_file_path}")
            
            # V√©rifier l'extension du fichier
            file_ext = Path(audio_file).suffix.lower()
            formats_supported_by_torchaudio = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac']
            
            # Si le format n'est pas directement support√© (ex: WMA), convertir avec pydub
            if file_ext not in formats_supported_by_torchaudio:
                print(f"‚ö†Ô∏è Format {file_ext} d√©tect√©, conversion en WAV temporaire...")
                try:
                    # Charger avec pydub (supporte plus de formats)
                    audio = AudioSegment.from_file(audio_file)
                    # Cr√©er un fichier temporaire WAV
                    temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                    audio.export(temp_wav.name, format='wav')
                    audio_file = temp_wav.name
                    print(f"‚úÖ Fichier converti en WAV: {audio_file}")
                except Exception as e:
                    print(f"‚ùå Erreur lors de la conversion: {e}")
                    return None, None, None, None, f"‚ùå Format {file_ext} non support√©. Veuillez convertir en MP3, WAV, FLAC, OGG ou M4A."
            
            try:
                # Essayer de charger avec torchaudio (sans torchcodec)
                wav, sr = torchaudio.load(audio_file, backend="soundfile")
            except Exception as e:
                # Si √©chec, utiliser librosa comme fallback
                print(f"‚ö†Ô∏è torchaudio √©chou√©, utilisation de librosa: {e}")
                import librosa
                audio_data, sr = librosa.load(audio_file, sr=None, mono=False)
                if len(audio_data.shape) == 1:
                    audio_data = audio_data.reshape(1, -1)  # Ajouter dimension channel
                wav = torch.from_numpy(audio_data).float()
            
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)  # Convertir en mono si st√©r√©o
        
        # Convertir au format attendu par Demucs
        wav = convert_audio(wav, sr, MODEL_SAMPLE_RATE, MODEL_CHANNELS)
        wav = wav.to(DEVICE)
        
        print(f"üîä Audio charg√©: {wav.shape}, sample_rate: {MODEL_SAMPLE_RATE}")
        
        # S√©parer les sources
        print("üîÑ S√©paration des sources en cours...")
        if progress:
            progress(0.3, desc="üîÑ Chargement et pr√©paration de l'audio...")
        
        with torch.no_grad():
            # Utiliser apply_model au lieu d'un appel direct
            if progress:
                progress(0.5, desc="üéµ S√©paration des sources avec Demucs...")
            sources = apply_model(model, wav[None], device=DEVICE, split=True, overlap=0.25, progress=False)
            sources = sources[0]  # Retirer dimension batch
        
        if progress:
            progress(0.8, desc="üíæ Sauvegarde des fichiers s√©par√©s...")
        
        # Les sources sont dans l'ordre: [drums, bass, other, vocals]
        drums, bass, other, vocals = sources
        
        # Utiliser le dossier outputs configur√© dans Docker
        output_dir = Path("/app/outputs")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Cr√©er un sous-dossier avec timestamp pour cette s√©paration
        import time
        timestamp = int(time.time())
        session_dir = output_dir / f"session_{timestamp}"
        session_dir.mkdir(parents=True, exist_ok=True)
        
        results = {}
        
        # Fonction helper pour sauvegarder avec soundfile (√©vite TorchCodec)
        def save_audio_safe(path, audio_tensor, sample_rate):
            """Sauvegarde un tensor audio en WAV en utilisant soundfile directement"""
            # Convertir en numpy array et transposer si n√©cessaire
            audio_np = audio_tensor.cpu().numpy()
            if len(audio_np.shape) == 3:
                audio_np = audio_np[0]  # Retirer dimension batch
            if len(audio_np.shape) == 2:
                # Transposer de (channels, samples) √† (samples, channels)
                audio_np = audio_np.T
            elif len(audio_np.shape) == 1:
                # Mono: ajouter dimension channel
                audio_np = audio_np.reshape(-1, 1)
            sf.write(str(path), audio_np, sample_rate)
        
        if stem == "all" or stem == "vocals":
            vocals_path = session_dir / "vocals.wav"
            save_audio_safe(vocals_path, vocals, MODEL_SAMPLE_RATE)
            results["vocals"] = str(vocals_path)
        
        if stem == "all" or stem == "drums":
            drums_path = session_dir / "drums.wav"
            save_audio_safe(drums_path, drums, MODEL_SAMPLE_RATE)
            results["drums"] = str(drums_path)
        
        if stem == "all" or stem == "bass":
            bass_path = session_dir / "bass.wav"
            save_audio_safe(bass_path, bass, MODEL_SAMPLE_RATE)
            results["bass"] = str(bass_path)
        
        if stem == "all" or stem == "other":
            other_path = session_dir / "other.wav"
            save_audio_safe(other_path, other, MODEL_SAMPLE_RATE)
            results["other"] = str(other_path)
        
        # Obtenir les informations du fichier
        file_info = get_audio_info(audio_file)
        
        if progress:
            progress(1.0, desc="‚úÖ Traitement termin√©!")
        
        # Retourner les r√©sultats selon le stem demand√©
        if stem == "all":
            return (
                results.get("vocals"),
                results.get("drums"),
                results.get("bass"),
                results.get("other"),
                "‚úÖ S√©paration compl√®te termin√©e! Tous les fichiers sont pr√™ts au t√©l√©chargement.",
                file_info
            )
        else:
            result_file = results.get(stem)
            if result_file:
                return result_file, None, None, None, f"‚úÖ {stem.capitalize()} extrait avec succ√®s!", file_info
            else:
                return None, None, None, None, f"‚ùå Erreur lors de l'extraction de {stem}", file_info
        
    except Exception as e:
        error_msg = f"‚ùå Erreur: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        file_info = get_audio_info(audio_file) if audio_file else ""
        return None, None, None, None, error_msg, file_info

# Interface Gradio
with gr.Blocks() as demo:
    
    gr.HTML("""
    <div class="main-header">
        <h1>üé§ Isolation Vocale par IA</h1>
        <p>S√©parez la voix, la batterie, la basse et les autres instruments de vos fichiers audio</p>
        <p style="font-size: 0.9em; opacity: 0.9;">Bas√© sur Demucs v4 - Mod√®le HTDemucs | Qualit√© professionnelle</p>
    </div>
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### üì§ Upload et Configuration")
            
            audio_input = gr.Audio(
                label="üìÅ Fichier Audio",
                type="filepath",
                sources=["upload", "microphone"],
                format="wav",
                show_label=True
            )
            
            # Informations sur le fichier
            file_info_display = gr.Textbox(
                label="‚ÑπÔ∏è Informations du fichier",
                interactive=False,
                value="Aucun fichier s√©lectionn√©",
                visible=True
            )
            
            # Pr√©visualisation de l'original
            original_preview = gr.Audio(
                label="üéµ Pr√©visualisation (Original)",
                type="filepath",
                visible=True,
                interactive=False
            )
            
            gr.Markdown("### ‚öôÔ∏è Options de s√©paration")
            
            stem_choice = gr.Radio(
                choices=[
                    ("üé§ Voix uniquement", "vocals"),
                    ("ü•Å Batterie uniquement", "drums"),
                    ("üé∏ Basse uniquement", "bass"),
                    ("üéπ Autres instruments", "other"),
                    ("üéµ Toutes les sources", "all")
                ],
                value="vocals",
                label="Source √† extraire",
                info="Choisissez quelle source vous souhaitez isoler"
            )
            
            separate_btn = gr.Button(
                "üéØ S√©parer les sources",
                variant="primary",
                size="lg"
            )
            
            status = gr.Textbox(
                label="üìä Statut",
                interactive=False,
                value="‚è≥ Pr√™t √† traiter un fichier audio"
            )
        
        with gr.Column(scale=1):
            gr.Markdown("### üéß R√©sultats de la s√©paration")
            
            # Visualisation des pistes avec waveforms et lecteurs audio
            tracks_visualization = gr.HTML(
                value="<div style='padding: 20px; text-align: center; color: #666;'><p>Les pistes trait√©es appara√Ætront ici avec leurs visualisations</p></div>",
                label="üìä Visualisation des pistes",
                visible=True
            )
            
            if True:  # Pour permettre plusieurs outputs conditionnels
                vocals_output = gr.Audio(
                    label="üé§ Voix isol√©e",
                    type="filepath",
                    visible=True
                )
                drums_output = gr.Audio(
                    label="ü•Å Batterie isol√©e",
                    type="filepath",
                    visible=False
                )
                bass_output = gr.Audio(
                    label="üé∏ Basse isol√©e",
                    type="filepath",
                    visible=False
                )
                other_output = gr.Audio(
                    label="üéπ Autres instruments isol√©s",
                    type="filepath",
                    visible=False
                )
            
            # Bouton de t√©l√©chargement en lot (visible seulement si "all")
            download_all_btn = gr.File(
                label="üì¶ T√©l√©charger toutes les sources",
                visible=False,
                file_count="multiple"
            )
    
    # Fonction pour g√©rer la visibilit√© des outputs
    def update_outputs_visibility(stem):
        if stem == "all":
            return (
                gr.update(visible=True),  # vocals
                gr.update(visible=True),  # drums
                gr.update(visible=True),  # bass
                gr.update(visible=True),  # other
            )
        else:
            return (
                gr.update(visible=(stem == "vocals")),  # vocals
                gr.update(visible=(stem == "drums")),  # drums
                gr.update(visible=(stem == "bass")),  # bass
                gr.update(visible=(stem == "other")),  # other
            )
    
    # Fonction pour mettre √† jour les informations du fichier
    def update_file_info(audio_file):
        info = get_audio_info(audio_file)
        return info, audio_file if audio_file else None
    
    # Fonction wrapper pour la s√©paration avec visualisation
    def process_separation(audio_file, stem, progress=gr.Progress()):
        # Gradio injecte automatiquement l'objet progress
        result = separate_sources(audio_file, stem, progress)
        
        vocals, drums, bass, other, status_msg, file_info = result
        
        # G√©n√©rer les waveforms pour chaque piste
        waveforms = {}
        if vocals:
            waveforms['vocals'] = generate_waveform_image(vocals, "üé§ Voix isol√©e", "#9333ea")
        if drums:
            waveforms['drums'] = generate_waveform_image(drums, "ü•Å Batterie isol√©e", "#ef4444")
        if bass:
            waveforms['bass'] = generate_waveform_image(bass, "üé∏ Basse isol√©e", "#3b82f6")
        if other:
            waveforms['other'] = generate_waveform_image(other, "üéπ Autres instruments", "#10b981")
        
        # Cr√©er le HTML de visualisation
        tracks_html = ""
        if stem == "all":
            if vocals:
                tracks_html += create_track_html(vocals, "Voix isol√©e", "üé§", "#9333ea", waveforms.get('vocals'))
            if drums:
                tracks_html += create_track_html(drums, "Batterie isol√©e", "ü•Å", "#ef4444", waveforms.get('drums'))
            if bass:
                tracks_html += create_track_html(bass, "Basse isol√©e", "üé∏", "#3b82f6", waveforms.get('bass'))
            if other:
                tracks_html += create_track_html(other, "Autres instruments", "üéπ", "#10b981", waveforms.get('other'))
        else:
            if stem == "vocals" and vocals:
                tracks_html = create_track_html(vocals, "Voix isol√©e", "üé§", "#9333ea", waveforms.get('vocals'))
            elif stem == "drums" and drums:
                tracks_html = create_track_html(drums, "Batterie isol√©e", "ü•Å", "#ef4444", waveforms.get('drums'))
            elif stem == "bass" and bass:
                tracks_html = create_track_html(bass, "Basse isol√©e", "üé∏", "#3b82f6", waveforms.get('bass'))
            elif stem == "other" and other:
                tracks_html = create_track_html(other, "Autres instruments", "üéπ", "#10b981", waveforms.get('other'))
        
        # Pr√©parer les fichiers pour le t√©l√©chargement en lot
        download_files = []
        if stem == "all":
            if vocals: download_files.append(vocals)
            if drums: download_files.append(drums)
            if bass: download_files.append(bass)
            if other: download_files.append(other)
        
        if stem == "all":
            return (
                vocals,
                drums,
                bass,
                other,
                status_msg,
                file_info,
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(visible=True),
                gr.update(value=download_files, visible=True) if download_files else gr.update(visible=False),
                gr.HTML(tracks_html) if tracks_html else gr.HTML(""),
            )
        else:
            visibility = update_outputs_visibility(stem)
            return (
                vocals if stem == "vocals" else None,
                drums if stem == "drums" else None,
                bass if stem == "bass" else None,
                other if stem == "other" else None,
                status_msg,
                file_info,
                *visibility,
                gr.update(visible=False),
                gr.HTML(tracks_html) if tracks_html else gr.HTML(""),
            )
    
    # Connecter les √©v√©nements
    # Mettre √† jour les informations du fichier quand un fichier est upload√©
    audio_input.change(
        fn=update_file_info,
        inputs=[audio_input],
        outputs=[file_info_display, original_preview]
    )
    
    stem_choice.change(
        fn=update_outputs_visibility,
        inputs=[stem_choice],
        outputs=[vocals_output, drums_output, bass_output, other_output]
    )
    
    separate_btn.click(
        fn=process_separation,
        inputs=[audio_input, stem_choice],
        outputs=[
            vocals_output,
            drums_output,
            bass_output,
            other_output,
            status,
            file_info_display,
            vocals_output,
            drums_output,
            bass_output,
            other_output,
            download_all_btn,
            tracks_visualization
        ]
    )
    
    # Exemples
    gr.Examples(
        examples=[],
        inputs=audio_input,
        label="Exemples (√† venir)"
    )
    
    # Informations et aide
    with gr.Accordion("‚ÑπÔ∏è Comment √ßa fonctionne ?", open=False):
        gr.Markdown("""
        Cette application utilise **Demucs v4** (Hybrid Transformer), un mod√®le d'IA de pointe pour la s√©paration de sources audio.
        
        **Fonctionnalit√©s :**
        - üé§ **Isolation vocale** : Extrait uniquement la voix d'un enregistrement
        - ü•Å **Isolation de batterie** : S√©pare la batterie du reste
        - üé∏ **Isolation de basse** : Extrait la ligne de basse
        - üéπ **Autres instruments** : Isole les autres instruments (guitares, synth√©s, etc.)
        - üéµ **S√©paration compl√®te** : Extrait toutes les sources en une fois
        
        **Formats support√©s :** MP3, WAV, M4A, OGG, FLAC, WMA (converti automatiquement)
        
        **Note :** 
        - Le traitement peut prendre quelques minutes selon la longueur du fichier audio
        - Les formats non support√©s par les navigateurs (comme WMA) sont automatiquement convertis
        - Les fichiers de sortie sont toujours en format WAV pour une compatibilit√© maximale
        - Vous pouvez pr√©visualiser l'original avant de traiter
        - Utilisez "Toutes les sources" pour obtenir tous les stems en une fois
        """)
    
    with gr.Accordion("üí° Conseils d'utilisation", open=False):
        gr.Markdown("""
        **Pour de meilleurs r√©sultats :**
        1. üì§ Uploadez un fichier audio de bonne qualit√© (WAV ou FLAC recommand√©)
        2. üéµ Pr√©visualisez l'original pour v√©rifier la qualit√©
        3. üéØ Choisissez la source √† extraire ou s√©lectionnez "Toutes les sources"
        4. ‚è≥ Attendez la fin du traitement (barre de progression affich√©e)
        5. üéß √âcoutez les r√©sultats et t√©l√©chargez ce dont vous avez besoin
        
        **Comparaison avec d'autres outils :**
        - **Demucs v4** offre une meilleure qualit√© que Spleeter
        - **HTDemucs** est optimis√© pour la s√©paration vocale
        - Les r√©sultats sont de qualit√© professionnelle, pr√™ts pour le mixage
        """)

# Lancer l'application
if __name__ == "__main__":
    # Configurer les dossiers pour Gradio
    upload_dir = Path("/app/uploads")
    output_dir = Path("/app/outputs")
    upload_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
        }
        .main-header {
            text-align: center;
            padding: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 15px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .main-header h1 {
            margin: 0 0 10px 0;
            font-size: 2.5em;
        }
        .main-header p {
            margin: 5px 0;
        }
        """
    )
